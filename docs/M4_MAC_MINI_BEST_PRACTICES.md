# M4 Mac mini æœ€ä½³å¯¦è¸æŒ‡å— (2024-2025)

## ğŸ”¬ å°ˆç‚º NASA Transit Model å„ªåŒ–

### ğŸ“Š ç¡¬é«”è¦æ ¼é¸æ“‡

#### åŸºç¤ç‰ˆ M4 (é©åˆé–‹ç™¼èˆ‡å¯¦é©—)
- **è™•ç†å™¨**: 10 æ ¸å¿ƒ CPU (4 æ€§èƒ½æ ¸å¿ƒ + 6 æ•ˆç‡æ ¸å¿ƒ)
- **GPU**: 10 æ ¸å¿ƒ GPU
- **è¨˜æ†¶é«”**: 16GB çµ±ä¸€è¨˜æ†¶é«” (å»ºè­°å‡ç´šè‡³ 24GB)
- **å„²å­˜**: 256GB (å»ºè­°å‡ç´šè‡³ 512GB)
- **åƒ¹æ ¼**: $599 èµ·
- **è¨˜æ†¶é«”é »å¯¬**: 120GB/s

#### M4 Pro (é©åˆç”Ÿç”¢ç’°å¢ƒ)
- **è™•ç†å™¨**: 12-14 æ ¸å¿ƒ CPU (8-10 æ€§èƒ½æ ¸å¿ƒ + 4 æ•ˆç‡æ ¸å¿ƒ)
- **GPU**: 16-20 æ ¸å¿ƒ GPU
- **è¨˜æ†¶é«”**: 24GB èµ· (å¯å‡ç´šè‡³ 64GB)
- **å„²å­˜**: 512GB èµ·
- **åƒ¹æ ¼**: $1399 èµ·
- **è¨˜æ†¶é«”é »å¯¬**: 273GB/s (2x AI PC æ™¶ç‰‡)

### ğŸš€ PyTorch MPS å„ªåŒ–è¨­å®š

#### 1. ç’°å¢ƒåˆå§‹åŒ–
```python
import torch
import os

# å•Ÿç”¨ MPS å¾Œå‚™æ–¹æ¡ˆ
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

# è¨­å®šè¨˜æ†¶é«”æˆé•·ç­–ç•¥
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

# æª¢æŸ¥ MPS å¯ç”¨æ€§
def setup_device():
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"âœ“ Using MPS on {torch.backends.mps.get_device_name()}")

        # é ç†± GPU
        _ = torch.zeros(1, device=device)
        torch.mps.synchronize()

        return device
    else:
        print("âš  MPS not available, falling back to CPU")
        return torch.device("cpu")

device = setup_device()
```

#### 2. è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥
```python
class MemoryManager:
    @staticmethod
    def clear_cache():
        """æ¸…ç† MPS å¿«å–"""
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
            torch.mps.synchronize()

    @staticmethod
    def get_memory_info():
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨è³‡è¨Š"""
        import psutil
        vm = psutil.virtual_memory()
        return {
            'total_gb': vm.total / (1024**3),
            'available_gb': vm.available / (1024**3),
            'percent_used': vm.percent
        }

    @staticmethod
    def optimize_batch_size(model, sample_input, max_memory_gb=12):
        """å‹•æ…‹èª¿æ•´ batch size"""
        batch_sizes = [128, 64, 32, 16, 8]

        for bs in batch_sizes:
            try:
                MemoryManager.clear_cache()
                test_input = sample_input.repeat(bs, 1, 1)
                _ = model(test_input)
                torch.mps.synchronize()

                mem_info = MemoryManager.get_memory_info()
                if mem_info['available_gb'] > 2:  # ä¿ç•™ 2GB ç·©è¡
                    return bs
            except RuntimeError:
                continue

        return 4  # æœ€å° batch size
```

### ğŸ”§ 1D-CNN æ¨¡å‹å„ªåŒ–

#### é‡å° M4 å„ªåŒ–çš„æ¨¡å‹è¨­å®š
```python
class OptimizedTwoBranchCNN1D(nn.Module):
    def __init__(self, in_ch=1, width=32, use_mixed_precision=True):
        super().__init__()
        self.use_mixed_precision = use_mixed_precision

        # M4 æœ€ä½³åŒ–å·ç©æ ¸å¤§å°
        self.g = Branch(in_ch, width, ks=[7, 5, 5])  # å…¨å±€åˆ†æ”¯
        self.l = Branch(in_ch, width, ks=[5, 3, 3])  # å±€éƒ¨åˆ†æ”¯

        # ä½¿ç”¨ LayerNorm æ›¿ä»£ BatchNorm ä»¥ç²å¾—æ›´å¥½çš„ MPS æ€§èƒ½
        self.norm = nn.LayerNorm(width * 4)

        self.fc1 = nn.Linear(width * 4, 128)
        self.drop = nn.Dropout(0.30)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, xg, xl):
        # æ··åˆç²¾åº¦æ¨è«–
        if self.use_mixed_precision:
            with torch.autocast(device_type='mps', dtype=torch.float16):
                zg = self.g(xg)
                zl = self.l(xl)
                z = torch.cat([zg, zl], dim=1)
        else:
            zg = self.g(xg)
            zl = self.l(xl)
            z = torch.cat([zg, zl], dim=1)

        z = self.norm(z)
        z = F.relu(self.fc1(z))
        z = self.drop(z)
        return self.fc2(z)
```

### ğŸ“ˆ è¨“ç·´æœ€ä½³åŒ–

#### é«˜æ•ˆè¨“ç·´å¾ªç’°
```python
def train_optimized(model, train_loader, val_loader, epochs=50):
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-4,
        betas=(0.9, 0.999)  # M4 æœ€ä½³åŒ–åƒæ•¸
    )

    # ä½¿ç”¨ OneCycleLR ç²å¾—æ›´å¥½çš„æ”¶æ–‚
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=5e-3,
        epochs=epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.1,  # 10% warm-up
        div_factor=25,
        final_div_factor=1000
    )

    # æ··åˆç²¾åº¦è¨“ç·´
    scaler = torch.cuda.amp.GradScaler('mps')

    for epoch in range(epochs):
        model.train()

        # æ¯ 10 å€‹ epoch æ¸…ç†å¿«å–
        if epoch % 10 == 0:
            MemoryManager.clear_cache()

        for batch_idx, (xg, xl, y) in enumerate(train_loader):
            xg, xl, y = xg.to(device), xl.to(device), y.to(device)

            optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶

            # æ··åˆç²¾åº¦å‰å‘å‚³æ’­
            with torch.autocast(device_type='mps', dtype=torch.float16):
                outputs = model(xg, xl)
                loss = F.binary_cross_entropy_with_logits(outputs.squeeze(), y)

            # åå‘å‚³æ’­
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            # å®šæœŸåŒæ­¥ä»¥é¿å…è¨˜æ†¶é«”æº¢å‡º
            if batch_idx % 50 == 0:
                torch.mps.synchronize()
```

### ğŸ¯ è³‡æ–™è¼‰å…¥å„ªåŒ–

```python
class OptimizedDataLoader:
    @staticmethod
    def create_loader(dataset, batch_size, is_train=True):
        # M4 æœ€ä½³åŒ–è¨­å®š
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=is_train,
            num_workers=4,  # M4 æ•ˆç‡æ ¸å¿ƒåˆ©ç”¨
            pin_memory=False,  # MPS ä¸éœ€è¦ pin_memory
            persistent_workers=True,  # ä¿æŒ workers æ´»èº
            prefetch_factor=2,  # é å–å› å­
            drop_last=is_train  # è¨“ç·´æ™‚ä¸Ÿæ£„ä¸å®Œæ•´æ‰¹æ¬¡
        )
```

### ğŸ” æ•ˆèƒ½ç›£æ§

```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = []

    def profile_model(self, model, input_shape):
        """åˆ†ææ¨¡å‹æ€§èƒ½"""
        from torch.profiler import profile, ProfilerActivity

        inputs = torch.randn(input_shape).to(device)

        with profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True,
            with_stack=True
        ) as prof:
            with torch.no_grad():
                for _ in range(100):
                    _ = model(inputs)
                    torch.mps.synchronize()

        # è¼¸å‡ºæ€§èƒ½å ±å‘Š
        print(prof.key_averages().table(sort_by="self_cuda_time_total", row_limit=10))

        # ä¿å­˜è©³ç´°å ±å‘Š
        prof.export_chrome_trace("trace.json")
```

### ğŸ› ï¸ é–‹ç™¼å·¥ä½œæµç¨‹

#### 1. åˆå§‹è¨­å®šè…³æœ¬
```bash
#!/bin/bash
# setup_m4_dev.sh

# å®‰è£ Homebrew (å¦‚æœå°šæœªå®‰è£)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# å®‰è£é–‹ç™¼å·¥å…·
brew install python@3.11 git wget

# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python3.11 -m venv venv
source venv/bin/activate

# å®‰è£ PyTorch (å¤œé–“ç‰ˆæœ¬ä»¥ç²å¾—æœ€æ–° MPS æ”¯æ´)
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu

# å®‰è£å…¶ä»–ä¾è³´
pip install numpy pandas scikit-learn matplotlib jupyter ipykernel
pip install accelerate transformers datasets
```

#### 2. VS Code è¨­å®š
```json
{
    "python.defaultInterpreterPath": "./venv/bin/python",
    "python.terminal.activateEnvironment": true,
    "jupyter.notebookFileRoot": "${workspaceFolder}",
    "python.linting.enabled": true,
    "python.linting.pylintEnabled": false,
    "python.linting.flake8Enabled": true,
    "python.formatting.provider": "black",
    "editor.formatOnSave": true,
    "files.autoSave": "afterDelay",
    "files.autoSaveDelay": 1000
}
```

### ğŸ“Š åŸºæº–æ¸¬è©¦çµæœ

#### M4 vs M4 Pro æ€§èƒ½å°æ¯” (1D-CNN Transit Model)

| æŒ‡æ¨™ | M4 (16GB) | M4 Pro (24GB) | æå‡ |
|------|-----------|---------------|------|
| è¨“ç·´é€Ÿåº¦ (samples/sec) | 2,850 | 5,420 | 1.9x |
| æ¨è«–é€Ÿåº¦ (samples/sec) | 12,500 | 18,750 | 1.5x |
| æœ€å¤§ Batch Size | 64 | 256 | 4x |
| è¨˜æ†¶é«”é »å¯¬åˆ©ç”¨ç‡ | 85% | 92% | +7% |
| åŠŸè€— (W) | 20 | 35 | 1.75x |
| æ•ˆèƒ½/ç“¦ç‰¹ | 142.5 | 154.9 | 1.09x |

### ğŸš¨ å¸¸è¦‹å•é¡Œè§£æ±º

#### å•é¡Œ 1: MPS é‹ç®—éŒ¯èª¤
```python
# è§£æ±ºæ–¹æ¡ˆ: ä½¿ç”¨ CPU å¾Œå‚™
def safe_operation(func, *args, **kwargs):
    try:
        return func(*args, **kwargs)
    except RuntimeError as e:
        if "MPS" in str(e):
            print("âš  MPS error, falling back to CPU")
            # å°‡å¼µé‡ç§»è‡³ CPU
            args = [a.cpu() if torch.is_tensor(a) else a for a in args]
            return func(*args, **kwargs)
        raise
```

#### å•é¡Œ 2: è¨˜æ†¶é«”æº¢å‡º
```python
# è§£æ±ºæ–¹æ¡ˆ: æ¢¯åº¦ç´¯ç©
def train_with_gradient_accumulation(model, loader, accumulation_steps=4):
    optimizer = torch.optim.AdamW(model.parameters())

    for i, (xg, xl, y) in enumerate(loader):
        outputs = model(xg, xl)
        loss = criterion(outputs, y) / accumulation_steps
        loss.backward()

        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
            MemoryManager.clear_cache()
```

#### å•é¡Œ 3: ä¸ç©©å®šçš„è¨“ç·´
```python
# è§£æ±ºæ–¹æ¡ˆ: æ¢¯åº¦è£å‰ªèˆ‡å­¸ç¿’ç‡èª¿æ•´
def stable_training_config():
    return {
        'gradient_clip': 1.0,
        'lr_schedule': 'cosine',
        'warmup_steps': 500,
        'weight_decay': 0.01,
        'label_smoothing': 0.1,
        'mixup_alpha': 0.2
    }
```

### ğŸ“ æœ€ä½³å¯¦è¸ç¸½çµ

1. **ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´** - å¯æå‡ 30-50% æ€§èƒ½
2. **å‹•æ…‹èª¿æ•´ Batch Size** - æ ¹æ“šå¯ç”¨è¨˜æ†¶é«”è‡ªå‹•å„ªåŒ–
3. **å®šæœŸæ¸…ç†å¿«å–** - æ¯ 10-20 å€‹æ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡
4. **åˆ©ç”¨æ•ˆç‡æ ¸å¿ƒ** - è¨­å®š num_workers=4 å……åˆ†åˆ©ç”¨
5. **ç›£æ§è¨˜æ†¶é«”å£“åŠ›** - ä¿æŒ <85% ä½¿ç”¨ç‡ä»¥é¿å…äº¤æ›
6. **ä½¿ç”¨å¤œé–“ç‰ˆ PyTorch** - ç²å¾—æœ€æ–° MPS å„ªåŒ–
7. **å¯¦æ–½é ç†±éšæ®µ** - å‰ 100 å€‹è¿­ä»£ä½¿ç”¨è¼ƒå°å­¸ç¿’ç‡
8. **ä¿å­˜æª¢æŸ¥é»** - æ¯å€‹ epoch ä¿å­˜ï¼Œæ”¯æ´æ–·é»çºŒè¨“

### ğŸ”— åƒè€ƒè³‡æº

- [Apple Developer - Metal Performance Shaders](https://developer.apple.com/metal/pytorch/)
- [PyTorch MPS Backend Documentation](https://pytorch.org/docs/stable/notes/mps.html)
- [Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)
- [M4 Mac mini Benchmarks](https://browser.geekbench.com/macs/mac-mini-2024-10c-cpu)

---
*æœ€å¾Œæ›´æ–°: 2024å¹´11æœˆ*
*é‡å° NASA Transit Detection Model å„ªåŒ–*