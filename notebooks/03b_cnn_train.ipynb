{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03b_cnn_train â€” 1D-CNN Training with Global/Local Views\n",
    "\n",
    "This notebook trains a 1D-CNN using BLS/TLS parameters and (time, flux) data.\n",
    "The model uses both global and local views of phase-folded light curves.\n",
    "\n",
    "Outputs:\n",
    "- `artifacts/cnn1d.pt` - Model weights\n",
    "- `artifacts/calibrator.joblib` - Probability calibration\n",
    "- `reports/metrics_cnn.json` - Performance metrics\n",
    "- `reports/calibration_cnn.png` - Calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from app.models.cnn1d import make_model\n",
    "from app.data.fold import Item, LightCurveViewsDataset\n",
    "from app.trainers.cnn1d_trainer import train\n",
    "from app.calibration.calibrate import run_and_save\n",
    "\n",
    "# Setup device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Transit Parameters\n",
    "\n",
    "Load the light curves and transit parameters from TLS/BLS search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transit search results from previous step\n",
    "transit_results_path = Path('../artifacts/transit_search_results.json')\n",
    "denoised_lc_path = Path('../artifacts/denoised_lc.npz')\n",
    "\n",
    "# Check if real data exists\n",
    "use_real_data = transit_results_path.exists() and denoised_lc_path.exists()\n",
    "\n",
    "if use_real_data:\n",
    "    print(\"Loading real data from TLS search...\")\n",
    "    with open(transit_results_path, 'r') as f:\n",
    "        transit_results = json.load(f)\n",
    "    \n",
    "    lc_data = np.load(denoised_lc_path)\n",
    "    t_real = lc_data['time']\n",
    "    flux_real = lc_data['flux_denoised']\n",
    "    \n",
    "    # Use TLS results if available, otherwise BLS\n",
    "    if transit_results.get('tls'):\n",
    "        period = transit_results['tls']['period']\n",
    "        t0 = transit_results['tls']['t0']\n",
    "        duration = transit_results['tls']['duration']\n",
    "        print(f\"Using TLS parameters: P={period:.4f}d, T0={t0:.4f}d, Dur={duration:.4f}d\")\n",
    "    elif transit_results.get('bls'):\n",
    "        period = transit_results['bls']['period']\n",
    "        t0 = transit_results['bls']['t0']\n",
    "        duration = transit_results['bls']['duration']\n",
    "        print(f\"Using BLS parameters: P={period:.4f}d, T0={t0:.4f}d, Dur={duration:.4f}d\")\n",
    "    else:\n",
    "        use_real_data = False\n",
    "        print(\"No transit parameters found, using synthetic data\")\n",
    "else:\n",
    "    print(\"No real data found, using synthetic data for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset for training\n",
    "def generate_synthetic_dataset(n_samples=1000, test_split=0.2, val_split=0.1):\n",
    "    \"\"\"Generate synthetic light curves with and without transits.\"\"\"\n",
    "    items = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Alternate between planet and non-planet\n",
    "        has_planet = i % 2 == 0\n",
    "        \n",
    "        if has_planet:\n",
    "            # Generate transit signal\n",
    "            period = np.random.uniform(1.5, 10.0)\n",
    "            t0 = np.random.uniform(0, 2.0)\n",
    "            duration = np.random.uniform(0.05, 0.15)  # In days\n",
    "            depth = np.random.uniform(0.0005, 0.003)\n",
    "            \n",
    "            n_points = 4096\n",
    "            t = np.linspace(0, period * 6, n_points)\n",
    "            flux = np.ones_like(t)\n",
    "            \n",
    "            # Add transits\n",
    "            phase = ((t - t0) / period) % 1.0\n",
    "            in_transit = phase < (duration / period)\n",
    "            flux[in_transit] -= depth\n",
    "            \n",
    "            # Add stellar variability\n",
    "            flux += 0.0001 * np.sin(2 * np.pi * t / 3.5)\n",
    "            \n",
    "            label = 1\n",
    "        else:\n",
    "            # Generate non-transit signal\n",
    "            period = np.random.uniform(1.5, 10.0)\n",
    "            t0 = 0.0\n",
    "            duration = 0.1\n",
    "            \n",
    "            n_points = 4096\n",
    "            t = np.linspace(0, 15, n_points)\n",
    "            flux = np.ones_like(t)\n",
    "            \n",
    "            # Add only stellar variability and spots\n",
    "            flux += 0.0002 * np.sin(2 * np.pi * t / np.random.uniform(2, 8))\n",
    "            flux += 0.0001 * np.sin(2 * np.pi * t / np.random.uniform(0.5, 2))\n",
    "            \n",
    "            label = 0\n",
    "        \n",
    "        # Add noise\n",
    "        noise_level = np.random.uniform(3e-4, 8e-4)\n",
    "        flux += np.random.default_rng(i).normal(0, noise_level, size=flux.shape)\n",
    "        \n",
    "        items.append(Item(\n",
    "            time=t,\n",
    "            flux=flux,\n",
    "            period=period,\n",
    "            t0=t0,\n",
    "            duration=duration,\n",
    "            label=label\n",
    "        ))\n",
    "    \n",
    "    return items\n",
    "\n",
    "# Generate or load dataset\n",
    "if use_real_data:\n",
    "    # For demo, create a small dataset using the real parameters\n",
    "    items = []\n",
    "    # Add the real light curve multiple times with slight variations\n",
    "    for i in range(100):\n",
    "        # Add noise variations\n",
    "        flux_variation = flux_real + np.random.normal(0, 2e-4, size=flux_real.shape)\n",
    "        items.append(Item(\n",
    "            time=t_real,\n",
    "            flux=flux_variation,\n",
    "            period=period,\n",
    "            t0=t0,\n",
    "            duration=duration,\n",
    "            label=1 if i < 50 else 0  # Half with planets, half without\n",
    "        ))\n",
    "    print(f\"Created {len(items)} samples from real data\")\n",
    "else:\n",
    "    # Generate synthetic dataset\n",
    "    items = generate_synthetic_dataset(n_samples=500)\n",
    "    print(f\"Generated {len(items)} synthetic samples\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = LightCurveViewsDataset(items)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Positive samples: {sum(item.label for item in items)}\")\n",
    "print(f\"Negative samples: {sum(1-item.label for item in items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.15)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    dataset, \n",
    "    [n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples\")\n",
    "print(f\"Validation: {len(val_ds)} samples\")\n",
    "print(f\"Test: {len(test_ds)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32 if device == 'cpu' else 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Global and Local Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample\n",
    "sample_idx = 0\n",
    "global_view, local_view, label = dataset[sample_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Global view\n",
    "axes[0].plot(global_view, 'b-', linewidth=0.5)\n",
    "axes[0].set_title(f'Global View (Label: {label})')\n",
    "axes[0].set_xlabel('Phase bin')\n",
    "axes[0].set_ylabel('Normalized flux')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Local view\n",
    "axes[1].plot(local_view, 'r-', linewidth=0.5)\n",
    "axes[1].set_title('Local View (Transit region)')\n",
    "axes[1].set_xlabel('Phase bin')\n",
    "axes[1].set_ylabel('Normalized flux')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Global view shape: {global_view.shape}\")\n",
    "print(f\"Local view shape: {local_view.shape}\")\n",
    "print(f\"Label: {label} ({'Planet' if label == 1 else 'No planet'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize and Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = make_model()\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'lr': 1e-3,\n",
    "    'max_epochs': 50,\n",
    "    'patience': 10,\n",
    "    'min_delta': 1e-4,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create output directories\n",
    "artifacts_dir = Path('../artifacts')\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nStarting training...\")\n",
    "metrics = train(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    device=device,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    lr=training_config['lr'],\n",
    "    max_epochs=training_config['max_epochs'],\n",
    "    patience=training_config['patience'],\n",
    "    workdir=str(artifacts_dir.parent)\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final validation loss: {metrics['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {metrics['val_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(metrics['train_loss'], label='Train Loss')\n",
    "axes[0].plot(metrics['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(metrics['train_acc'], label='Train Acc')\n",
    "axes[1].plot(metrics['val_acc'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / 'training_history_cnn.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = make_model()\n",
    "model.load_state_dict(torch.load(artifacts_dir / 'cnn1d.pt', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_preds = []\n",
    "test_probs = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for global_batch, local_batch, label_batch in test_loader:\n",
    "        # Move to device\n",
    "        global_batch = torch.tensor(global_batch, dtype=torch.float32).to(device)\n",
    "        local_batch = torch.tensor(local_batch, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(global_batch, local_batch).squeeze(1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs > 0.5).float()\n",
    "        \n",
    "        # Store results\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "        test_preds.append(preds.cpu().numpy())\n",
    "        test_labels.append(label_batch.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "test_probs = np.concatenate(test_probs)\n",
    "test_preds = np.concatenate(test_preds)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "test_metrics = {\n",
    "    'accuracy': accuracy_score(test_labels, test_preds),\n",
    "    'precision': precision_score(test_labels, test_preds),\n",
    "    'recall': recall_score(test_labels, test_preds),\n",
    "    'f1': f1_score(test_labels, test_preds),\n",
    "    'roc_auc': roc_auc_score(test_labels, test_probs)\n",
    "}\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Probability Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform probability calibration on validation set\n",
    "val_probs = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for global_batch, local_batch, label_batch in val_loader:\n",
    "        global_batch = torch.tensor(global_batch, dtype=torch.float32).to(device)\n",
    "        local_batch = torch.tensor(local_batch, dtype=torch.float32).to(device)\n",
    "        \n",
    "        logits = model(global_batch, local_batch).squeeze(1)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "        val_labels.append(label_batch.numpy())\n",
    "\n",
    "val_probs = np.concatenate(val_probs)\n",
    "val_labels = np.concatenate(val_labels)\n",
    "\n",
    "# Run calibration\n",
    "print(\"\\nRunning probability calibration...\")\n",
    "cal_info = run_and_save(\n",
    "    val_labels, \n",
    "    val_probs, \n",
    "    out_dir=str(artifacts_dir), \n",
    "    method='isotonic'\n",
    ")\n",
    "\n",
    "print(f\"Calibration method: {cal_info['method']}\")\n",
    "print(f\"ECE before: {cal_info['ece_before']:.4f}\")\n",
    "print(f\"ECE after: {cal_info['ece_after']:.4f}\")\n",
    "print(f\"Brier score before: {cal_info['brier_before']:.4f}\")\n",
    "print(f\"Brier score after: {cal_info['brier_after']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ROC and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import joblib\n",
    "\n",
    "# Load calibrator\n",
    "calibrator = joblib.load(artifacts_dir / 'calibrator.joblib')\n",
    "\n",
    "# Calibrate test probabilities\n",
    "test_probs_cal = calibrator.transform(test_probs.reshape(-1, 1)).ravel()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_probs_cal)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# PR curve\n",
    "precision, recall, _ = precision_recall_curve(test_labels, test_probs_cal)\n",
    "pr_auc = average_precision_score(test_labels, test_probs_cal)\n",
    "\n",
    "# Plot curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'CNN (AUC = {roc_auc:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR\n",
    "axes[1].plot(recall, precision, 'r-', linewidth=2, label=f'CNN (AP = {pr_auc:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(reports_dir / 'curves_cnn.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics\n",
    "final_metrics = {\n",
    "    'model': 'CNN1D',\n",
    "    'training': {\n",
    "        'epochs': len(metrics['train_loss']),\n",
    "        'batch_size': training_config['batch_size'],\n",
    "        'learning_rate': training_config['lr'],\n",
    "        'device': device,\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params\n",
    "    },\n",
    "    'dataset': {\n",
    "        'total_samples': n_total,\n",
    "        'train_samples': n_train,\n",
    "        'val_samples': n_val,\n",
    "        'test_samples': n_test\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_accuracy': float(test_metrics['accuracy']),\n",
    "        'test_precision': float(test_metrics['precision']),\n",
    "        'test_recall': float(test_metrics['recall']),\n",
    "        'test_f1': float(test_metrics['f1']),\n",
    "        'test_roc_auc': float(test_metrics['roc_auc']),\n",
    "        'test_pr_auc': float(pr_auc),\n",
    "        'val_loss_final': float(metrics['val_loss'][-1]),\n",
    "        'val_acc_final': float(metrics['val_acc'][-1])\n",
    "    },\n",
    "    'calibration': {\n",
    "        'method': cal_info['method'],\n",
    "        'ece_before': float(cal_info['ece_before']),\n",
    "        'ece_after': float(cal_info['ece_after']),\n",
    "        'brier_before': float(cal_info['brier_before']),\n",
    "        'brier_after': float(cal_info['brier_after'])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to JSON\n",
    "metrics_file = reports_dir / 'metrics_cnn.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetrics saved to {metrics_file}\")\n",
    "print(\"\\nFinal CNN Performance Summary:\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']:.3f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.3f}\")\n",
    "print(f\"  Recall: {test_metrics['recall']:.3f}\")\n",
    "print(f\"  F1 Score: {test_metrics['f1']:.3f}\")\n",
    "print(f\"  ROC-AUC: {test_metrics['roc_auc']:.3f}\")\n",
    "print(f\"  PR-AUC: {pr_auc:.3f}\")\n",
    "print(f\"  ECE (calibrated): {cal_info['ece_after']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test inference speed\n",
    "model.eval()\n",
    "batch_sizes = [1, 8, 32, 64]\n",
    "latency_results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Create dummy input\n",
    "    dummy_global = torch.randn(bs, 256).to(device)\n",
    "    dummy_local = torch.randn(bs, 64).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_global, dummy_local)\n",
    "    \n",
    "    # Time inference\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    n_iterations = 100\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_global, dummy_local)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    avg_latency = (end - start) / n_iterations * 1000  # Convert to ms\n",
    "    throughput = bs * n_iterations / (end - start)\n",
    "    \n",
    "    latency_results[bs] = {\n",
    "        'latency_ms': avg_latency,\n",
    "        'throughput_samples_per_sec': throughput\n",
    "    }\n",
    "    \n",
    "    print(f\"Batch size {bs:3d}: {avg_latency:.2f} ms/batch, {throughput:.1f} samples/sec\")\n",
    "\n",
    "# Add to final metrics\n",
    "final_metrics['inference_speed'] = latency_results\n",
    "\n",
    "# Save updated metrics\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully trained a 1D-CNN model for exoplanet detection using:\n",
    "- Global and local phase-folded views\n",
    "- TLS/BLS transit parameters\n",
    "- GPU acceleration when available\n",
    "- Probability calibration for reliable confidence scores\n",
    "\n",
    "Key outputs:\n",
    "- âœ… Model weights: `artifacts/cnn1d.pt`\n",
    "- âœ… Calibrator: `artifacts/calibrator.joblib`\n",
    "- âœ… Metrics: `reports/metrics_cnn.json`\n",
    "- âœ… Plots: `reports/training_history_cnn.png`, `reports/curves_cnn.png`, `reports/calibration_cnn.png`\n",
    "\n",
    "The model is now ready for inference on new data (see notebook 04)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}