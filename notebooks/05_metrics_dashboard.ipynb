{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_metrics_dashboard ‚Äî Model Comparison Dashboard\n",
    "\n",
    "This notebook provides a comprehensive comparison between different models:\n",
    "- **CNN**: 1D Convolutional Neural Network with global/local views\n",
    "- **XGBoost**: Gradient Boosting with engineered features\n",
    "- **Other models**: Random Forest, SVM, etc. (if available)\n",
    "\n",
    "Metrics compared:\n",
    "- PR-AUC / ROC-AUC\n",
    "- Calibration (ECE, Brier Score)\n",
    "- Inference Latency\n",
    "- Model Size and Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check for plotly (for interactive plots)\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    from plotly.subplots import make_subplots\n",
    "    plotly_available = True\n",
    "except ImportError:\n",
    "    plotly_available = False\n",
    "    print(\"‚ö† Plotly not available; interactive plots disabled\")\n",
    "\n",
    "print(f\"Dashboard initialized\")\n",
    "print(f\"Interactive plots: {'‚úì Available' if plotly_available else '‚úó Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics(reports_dir='../reports'):\n",
    "    \"\"\"Load metrics from all available models.\"\"\"\n",
    "    reports_path = Path(reports_dir)\n",
    "    metrics = {}\n",
    "    \n",
    "    # Patterns to search for\n",
    "    patterns = [\n",
    "        'metrics_cnn.json',\n",
    "        'metrics_xgb.json',\n",
    "        'metrics_xgboost.json',\n",
    "        'metrics_rf.json',\n",
    "        'metrics_svm.json',\n",
    "        'inference_metrics_*.json'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for file_path in reports_path.glob(pattern):\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    \n",
    "                # Extract model name\n",
    "                if 'model' in data:\n",
    "                    model_name = data['model']\n",
    "                elif 'model_type' in data:\n",
    "                    model_name = data['model_type']\n",
    "                else:\n",
    "                    # Infer from filename\n",
    "                    model_name = file_path.stem.replace('metrics_', '').replace('inference_metrics_', '').upper()\n",
    "                \n",
    "                metrics[model_name] = data\n",
    "                print(f\"‚úì Loaded metrics for {model_name} from {file_path.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Failed to load {file_path}: {e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Load all available metrics\n",
    "all_metrics = load_metrics()\n",
    "\n",
    "if not all_metrics:\n",
    "    print(\"\\n‚ö† No metrics found. Generating synthetic data for demonstration...\")\n",
    "    # Generate synthetic metrics for demo\n",
    "    all_metrics = {\n",
    "        'CNN': {\n",
    "            'model': 'CNN1D',\n",
    "            'performance': {\n",
    "                'test_accuracy': 0.945,\n",
    "                'test_precision': 0.932,\n",
    "                'test_recall': 0.958,\n",
    "                'test_f1': 0.945,\n",
    "                'test_roc_auc': 0.982,\n",
    "                'test_pr_auc': 0.978\n",
    "            },\n",
    "            'calibration': {\n",
    "                'ece_before': 0.082,\n",
    "                'ece_after': 0.021,\n",
    "                'brier_before': 0.095,\n",
    "                'brier_after': 0.052\n",
    "            },\n",
    "            'inference_speed': {\n",
    "                '1': {'latency_ms': 2.3, 'throughput_samples_per_sec': 435},\n",
    "                '32': {'latency_ms': 8.7, 'throughput_samples_per_sec': 3678}\n",
    "            }\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': 'XGBoost',\n",
    "            'performance': {\n",
    "                'test_accuracy': 0.912,\n",
    "                'test_precision': 0.895,\n",
    "                'test_recall': 0.931,\n",
    "                'test_f1': 0.913,\n",
    "                'test_roc_auc': 0.958,\n",
    "                'test_pr_auc': 0.952\n",
    "            },\n",
    "            'calibration': {\n",
    "                'ece_before': 0.125,\n",
    "                'ece_after': 0.038,\n",
    "                'brier_before': 0.132,\n",
    "                'brier_after': 0.071\n",
    "            },\n",
    "            'inference_speed': {\n",
    "                '1': {'latency_ms': 0.8, 'throughput_samples_per_sec': 1250},\n",
    "                '32': {'latency_ms': 3.2, 'throughput_samples_per_sec': 10000}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"\\nLoaded metrics for {len(all_metrics)} models: {list(all_metrics.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    row = {'Model': model_name}\n",
    "    \n",
    "    # Performance metrics\n",
    "    if 'performance' in metrics:\n",
    "        perf = metrics['performance']\n",
    "        row['Accuracy'] = perf.get('test_accuracy', 0) * 100\n",
    "        row['Precision'] = perf.get('test_precision', 0) * 100\n",
    "        row['Recall'] = perf.get('test_recall', 0) * 100\n",
    "        row['F1'] = perf.get('test_f1', 0) * 100\n",
    "        row['ROC-AUC'] = perf.get('test_roc_auc', 0)\n",
    "        row['PR-AUC'] = perf.get('test_pr_auc', 0)\n",
    "    \n",
    "    # Calibration metrics\n",
    "    if 'calibration' in metrics:\n",
    "        cal = metrics['calibration']\n",
    "        row['ECE (before)'] = cal.get('ece_before', 0)\n",
    "        row['ECE (after)'] = cal.get('ece_after', 0)\n",
    "        row['Brier (after)'] = cal.get('brier_after', 0)\n",
    "    \n",
    "    # Inference speed (batch size 32)\n",
    "    if 'inference_speed' in metrics:\n",
    "        if '32' in metrics['inference_speed']:\n",
    "            speed = metrics['inference_speed']['32']\n",
    "            row['Latency (ms)'] = speed.get('latency_ms', 0)\n",
    "            row['Throughput'] = speed.get('throughput_samples_per_sec', 0)\n",
    "    \n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Sort by ROC-AUC\n",
    "if 'ROC-AUC' in comparison_df.columns:\n",
    "    comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "# Display with formatting\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Format for display\n",
    "display_df = comparison_df.copy()\n",
    "\n",
    "# Format percentage columns\n",
    "pct_cols = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "for col in pct_cols:\n",
    "    if col in display_df.columns:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "# Format float columns\n",
    "float_cols = ['ROC-AUC', 'PR-AUC', 'ECE (before)', 'ECE (after)', 'Brier (after)']\n",
    "for col in float_cols:\n",
    "    if col in display_df.columns:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "# Format latency\n",
    "if 'Latency (ms)' in display_df.columns:\n",
    "    display_df['Latency (ms)'] = display_df['Latency (ms)'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "# Format throughput\n",
    "if 'Throughput' in display_df.columns:\n",
    "    display_df['Throughput'] = display_df['Throughput'].apply(lambda x: f\"{x:.0f}/s\")\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "if len(comparison_df) > 1:\n",
    "    print(\"Best Performers:\")\n",
    "    for col in ['ROC-AUC', 'PR-AUC', 'F1']:\n",
    "        if col in comparison_df.columns:\n",
    "            best_model = comparison_df.loc[comparison_df[col].idxmax(), 'Model']\n",
    "            best_value = comparison_df[col].max()\n",
    "            if col == 'F1':\n",
    "                print(f\"  {col:12s}: {best_model} ({best_value:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  {col:12s}: {best_model} ({best_value:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# 1. ROC-AUC and PR-AUC comparison\n",
    "if 'ROC-AUC' in comparison_df.columns and 'PR-AUC' in comparison_df.columns:\n",
    "    models = comparison_df['Model'].values\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, comparison_df['ROC-AUC'], width, label='ROC-AUC', color='skyblue')\n",
    "    axes[0,0].bar(x + width/2, comparison_df['PR-AUC'], width, label='PR-AUC', color='lightcoral')\n",
    "    axes[0,0].set_xlabel('Model')\n",
    "    axes[0,0].set_ylabel('AUC Score')\n",
    "    axes[0,0].set_title('ROC-AUC vs PR-AUC')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(models)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_ylim([0, 1])\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. F1 Score comparison\n",
    "if 'F1' in comparison_df.columns:\n",
    "    axes[0,1].bar(comparison_df['Model'], comparison_df['F1'], color='mediumseagreen')\n",
    "    axes[0,1].set_xlabel('Model')\n",
    "    axes[0,1].set_ylabel('F1 Score (%)')\n",
    "    axes[0,1].set_title('F1 Score Comparison')\n",
    "    axes[0,1].set_ylim([0, 100])\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_df['F1']):\n",
    "        axes[0,1].text(i, v + 1, f'{v:.1f}%', ha='center')\n",
    "\n",
    "# 3. Calibration comparison (ECE)\n",
    "if 'ECE (before)' in comparison_df.columns and 'ECE (after)' in comparison_df.columns:\n",
    "    models = comparison_df['Model'].values\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,2].bar(x - width/2, comparison_df['ECE (before)'], width, label='Before Cal.', color='salmon')\n",
    "    axes[0,2].bar(x + width/2, comparison_df['ECE (after)'], width, label='After Cal.', color='lightgreen')\n",
    "    axes[0,2].set_xlabel('Model')\n",
    "    axes[0,2].set_ylabel('ECE')\n",
    "    axes[0,2].set_title('Expected Calibration Error')\n",
    "    axes[0,2].set_xticks(x)\n",
    "    axes[0,2].set_xticklabels(models)\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Precision vs Recall\n",
    "if 'Precision' in comparison_df.columns and 'Recall' in comparison_df.columns:\n",
    "    axes[1,0].scatter(comparison_df['Recall'], comparison_df['Precision'], s=200)\n",
    "    for i, model in enumerate(comparison_df['Model']):\n",
    "        axes[1,0].annotate(model, \n",
    "                          (comparison_df.iloc[i]['Recall'], comparison_df.iloc[i]['Precision']),\n",
    "                          textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    axes[1,0].set_xlabel('Recall (%)')\n",
    "    axes[1,0].set_ylabel('Precision (%)')\n",
    "    axes[1,0].set_title('Precision vs Recall')\n",
    "    axes[1,0].set_xlim([0, 100])\n",
    "    axes[1,0].set_ylim([0, 100])\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Latency comparison\n",
    "if 'Latency (ms)' in comparison_df.columns:\n",
    "    axes[1,1].bar(comparison_df['Model'], comparison_df['Latency (ms)'], color='plum')\n",
    "    axes[1,1].set_xlabel('Model')\n",
    "    axes[1,1].set_ylabel('Latency (ms)')\n",
    "    axes[1,1].set_title('Inference Latency (Batch=32)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_df['Latency (ms)']):\n",
    "        axes[1,1].text(i, v + 0.1, f'{v:.1f}', ha='center')\n",
    "\n",
    "# 6. Throughput comparison\n",
    "if 'Throughput' in comparison_df.columns:\n",
    "    axes[1,2].bar(comparison_df['Model'], comparison_df['Throughput'], color='gold')\n",
    "    axes[1,2].set_xlabel('Model')\n",
    "    axes[1,2].set_ylabel('Samples/sec')\n",
    "    axes[1,2].set_title('Inference Throughput (Batch=32)')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale if values vary greatly\n",
    "    if comparison_df['Throughput'].max() / comparison_df['Throughput'].min() > 10:\n",
    "        axes[1,2].set_yscale('log')\n",
    "\n",
    "plt.suptitle('Model Performance Dashboard', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/model_comparison_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Dashboard saved to ../reports/model_comparison_dashboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Dashboard (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plotly_available and len(comparison_df) > 0:\n",
    "    # Create interactive subplot\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=(\n",
    "            'ROC-AUC vs PR-AUC', 'F1 Score', 'Calibration (ECE)',\n",
    "            'Precision vs Recall', 'Inference Latency', 'Throughput'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}],\n",
    "            [{'type': 'scatter'}, {'type': 'bar'}, {'type': 'bar'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ROC-AUC vs PR-AUC\n",
    "    if 'ROC-AUC' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(name='ROC-AUC', x=comparison_df['Model'], y=comparison_df['ROC-AUC'],\n",
    "                  marker_color='lightblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Bar(name='PR-AUC', x=comparison_df['Model'], y=comparison_df['PR-AUC'],\n",
    "                  marker_color='lightcoral'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # F1 Score\n",
    "    if 'F1' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['Model'], y=comparison_df['F1'],\n",
    "                  marker_color='mediumseagreen', showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # ECE\n",
    "    if 'ECE (after)' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['Model'], y=comparison_df['ECE (after)'],\n",
    "                  marker_color='lightgreen', showlegend=False),\n",
    "            row=1, col=3\n",
    "        )\n",
    "    \n",
    "    # Precision vs Recall\n",
    "    if 'Precision' in comparison_df.columns and 'Recall' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=comparison_df['Recall'], y=comparison_df['Precision'],\n",
    "                mode='markers+text',\n",
    "                text=comparison_df['Model'],\n",
    "                textposition='top center',\n",
    "                marker=dict(size=15, color=range(len(comparison_df))),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Latency\n",
    "    if 'Latency (ms)' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['Model'], y=comparison_df['Latency (ms)'],\n",
    "                  marker_color='plum', showlegend=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Throughput\n",
    "    if 'Throughput' in comparison_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=comparison_df['Model'], y=comparison_df['Throughput'],\n",
    "                  marker_color='gold', showlegend=False),\n",
    "            row=2, col=3\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        showlegend=True,\n",
    "        title_text=\"<b>Interactive Model Comparison Dashboard</b>\",\n",
    "        title_font_size=18\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Recall (%)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Precision (%)\", row=2, col=1)\n",
    "    \n",
    "    # Save and show\n",
    "    fig.write_html('../reports/interactive_dashboard.html')\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\n‚úì Interactive dashboard saved to ../reports/interactive_dashboard.html\")\n",
    "else:\n",
    "    if not plotly_available:\n",
    "        print(\"\\n‚ö† Plotly not available. Install with: pip install plotly\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† No data available for interactive dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade-off analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(comparison_df) > 1:\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        model = row['Model']\n",
    "        print(f\"\\n{model}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Strengths\n",
    "        strengths = []\n",
    "        if 'ROC-AUC' in row and row['ROC-AUC'] == comparison_df['ROC-AUC'].max():\n",
    "            strengths.append(\"Best ROC-AUC\")\n",
    "        if 'F1' in row and row['F1'] == comparison_df['F1'].max():\n",
    "            strengths.append(\"Best F1 Score\")\n",
    "        if 'Latency (ms)' in row and row['Latency (ms)'] == comparison_df['Latency (ms)'].min():\n",
    "            strengths.append(\"Lowest latency\")\n",
    "        if 'Throughput' in row and row['Throughput'] == comparison_df['Throughput'].max():\n",
    "            strengths.append(\"Highest throughput\")\n",
    "        if 'ECE (after)' in row and row['ECE (after)'] == comparison_df['ECE (after)'].min():\n",
    "            strengths.append(\"Best calibrated\")\n",
    "        \n",
    "        if strengths:\n",
    "            print(f\"  Strengths: {', '.join(strengths)}\")\n",
    "        \n",
    "        # Key metrics\n",
    "        if 'ROC-AUC' in row:\n",
    "            print(f\"  ROC-AUC: {row['ROC-AUC']:.3f}\")\n",
    "        if 'F1' in row:\n",
    "            print(f\"  F1 Score: {row['F1']:.1f}%\")\n",
    "        if 'Latency (ms)' in row:\n",
    "            print(f\"  Latency: {row['Latency (ms)']:.1f} ms\")\n",
    "        if 'Throughput' in row:\n",
    "            print(f\"  Throughput: {row['Throughput']:.0f} samples/sec\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(comparison_df) > 0:\n",
    "    # Best for accuracy\n",
    "    if 'ROC-AUC' in comparison_df.columns:\n",
    "        best_acc = comparison_df.loc[comparison_df['ROC-AUC'].idxmax(), 'Model']\n",
    "        print(f\"\\nüìä For maximum accuracy: Use {best_acc}\")\n",
    "    \n",
    "    # Best for speed\n",
    "    if 'Latency (ms)' in comparison_df.columns:\n",
    "        best_speed = comparison_df.loc[comparison_df['Latency (ms)'].idxmin(), 'Model']\n",
    "        print(f\"‚ö° For real-time inference: Use {best_speed}\")\n",
    "    \n",
    "    # Best balanced\n",
    "    if 'F1' in comparison_df.columns:\n",
    "        best_f1 = comparison_df.loc[comparison_df['F1'].idxmax(), 'Model']\n",
    "        print(f\"‚öñÔ∏è  For balanced performance: Use {best_f1}\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    if 'CNN' in comparison_df['Model'].values:\n",
    "        print(\"\\nüîç CNN: Best for complex patterns, requires GPU for optimal performance\")\n",
    "    if 'XGBoost' in comparison_df['Model'].values:\n",
    "        print(\"üå≤ XGBoost: Good balance of accuracy and speed, works well on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "summary = {\n",
    "    'n_models': len(comparison_df),\n",
    "    'models': list(comparison_df['Model'].values),\n",
    "    'best_performers': {},\n",
    "    'comparison_table': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Find best performers\n",
    "metrics_to_check = [\n",
    "    ('ROC-AUC', 'max'),\n",
    "    ('PR-AUC', 'max'),\n",
    "    ('F1', 'max'),\n",
    "    ('Accuracy', 'max'),\n",
    "    ('Latency (ms)', 'min'),\n",
    "    ('Throughput', 'max'),\n",
    "    ('ECE (after)', 'min')\n",
    "]\n",
    "\n",
    "for metric, op in metrics_to_check:\n",
    "    if metric in comparison_df.columns:\n",
    "        if op == 'max':\n",
    "            best_idx = comparison_df[metric].idxmax()\n",
    "            best_value = comparison_df[metric].max()\n",
    "        else:\n",
    "            best_idx = comparison_df[metric].idxmin()\n",
    "            best_value = comparison_df[metric].min()\n",
    "        \n",
    "        summary['best_performers'][metric] = {\n",
    "            'model': comparison_df.loc[best_idx, 'Model'],\n",
    "            'value': float(best_value)\n",
    "        }\n",
    "\n",
    "# Save summary\n",
    "summary_file = Path('../reports/model_comparison_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Summary report saved to {summary_file}\")\n",
    "\n",
    "# Also save comparison table as CSV\n",
    "csv_file = Path('../reports/model_comparison_table.csv')\n",
    "comparison_df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úì Comparison table saved to {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Dashboard generation complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  üìä model_comparison_dashboard.png - Visual comparisons\")\n",
    "print(\"  üìà interactive_dashboard.html - Interactive plots (if plotly available)\")\n",
    "print(\"  üìÑ model_comparison_summary.json - Detailed metrics\")\n",
    "print(\"  üìã model_comparison_table.csv - Comparison table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This dashboard provides comprehensive model comparison including:\n",
    "\n",
    "1. **Performance Metrics**: Accuracy, Precision, Recall, F1, ROC-AUC, PR-AUC\n",
    "2. **Calibration Analysis**: ECE and Brier scores before/after calibration\n",
    "3. **Efficiency Metrics**: Inference latency and throughput\n",
    "4. **Trade-off Analysis**: Strengths and weaknesses of each model\n",
    "5. **Recommendations**: Which model to use for different scenarios\n",
    "\n",
    "The dashboard supports multiple models and automatically adapts to available data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}