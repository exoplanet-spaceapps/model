{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_newdata_inference — Multi-Model Inference with Auto-Detection\n",
    "\n",
    "This notebook supports inference using multiple models:\n",
    "- **CNN Model**: If `artifacts/cnn1d.pt` + `artifacts/calibrator.joblib` exist\n",
    "- **XGBoost Model**: If `artifacts/xgboost_model.pkl` exists\n",
    "- **Other Models**: Extensible to support additional models\n",
    "\n",
    "Output format remains consistent regardless of the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing optional dependencies\n",
    "try:\n",
    "    import torch\n",
    "    torch_available = True\n",
    "except ImportError:\n",
    "    torch = None\n",
    "    torch_available = False\n",
    "    print(\"⚠ PyTorch not available; CNN inference disabled.\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "    xgb_available = False\n",
    "    print(\"⚠ XGBoost not available; XGBoost inference disabled.\")\n",
    "\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Check device availability for PyTorch\n",
    "if torch_available:\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"✓ Using Apple MPS\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"✓ Using CPU for PyTorch\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "print(f\"\\nAvailable backends:\")\n",
    "print(f\"  PyTorch: {torch_available}\")\n",
    "print(f\"  XGBoost: {xgb_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Detection and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"Unified model loader with automatic detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, artifacts_dir='../artifacts'):\n",
    "        self.artifacts_dir = Path(artifacts_dir)\n",
    "        self.models = {}\n",
    "        self.active_model = None\n",
    "        \n",
    "    def detect_available_models(self):\n",
    "        \"\"\"Detect which models are available.\"\"\"\n",
    "        available = []\n",
    "        \n",
    "        # Check for CNN model\n",
    "        cnn_path = self.artifacts_dir / 'cnn1d.pt'\n",
    "        cal_path = self.artifacts_dir / 'calibrator.joblib'\n",
    "        if cnn_path.exists() and cal_path.exists() and torch_available:\n",
    "            available.append('CNN')\n",
    "            print(f\"✓ CNN model found: {cnn_path}\")\n",
    "            \n",
    "        # Check for XGBoost model\n",
    "        xgb_paths = [\n",
    "            self.artifacts_dir / 'xgboost_model.pkl',\n",
    "            self.artifacts_dir / 'xgb_model.pkl',\n",
    "            self.artifacts_dir / 'model_xgb.pkl'\n",
    "        ]\n",
    "        for xgb_path in xgb_paths:\n",
    "            if xgb_path.exists() and xgb_available:\n",
    "                available.append('XGBoost')\n",
    "                print(f\"✓ XGBoost model found: {xgb_path}\")\n",
    "                break\n",
    "                \n",
    "        # Check for scikit-learn models\n",
    "        sklearn_paths = [\n",
    "            self.artifacts_dir / 'sklearn_model.pkl',\n",
    "            self.artifacts_dir / 'rf_model.pkl',\n",
    "            self.artifacts_dir / 'svm_model.pkl'\n",
    "        ]\n",
    "        for sklearn_path in sklearn_paths:\n",
    "            if sklearn_path.exists():\n",
    "                available.append('sklearn')\n",
    "                print(f\"✓ Scikit-learn model found: {sklearn_path}\")\n",
    "                break\n",
    "                \n",
    "        return available\n",
    "    \n",
    "    def load_cnn_model(self):\n",
    "        \"\"\"Load CNN model and calibrator.\"\"\"\n",
    "        if not torch_available:\n",
    "            raise RuntimeError(\"PyTorch not available\")\n",
    "            \n",
    "        try:\n",
    "            from app.models.cnn1d import make_model\n",
    "            \n",
    "            # Load model\n",
    "            model = make_model()\n",
    "            model_path = self.artifacts_dir / 'cnn1d.pt'\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Load calibrator\n",
    "            cal_path = self.artifacts_dir / 'calibrator.joblib'\n",
    "            calibrator = joblib.load(cal_path)\n",
    "            \n",
    "            self.models['CNN'] = {\n",
    "                'model': model,\n",
    "                'calibrator': calibrator,\n",
    "                'type': 'CNN'\n",
    "            }\n",
    "            \n",
    "            print(\"✓ CNN model loaded successfully\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load CNN model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_xgboost_model(self):\n",
    "        \"\"\"Load XGBoost model.\"\"\"\n",
    "        if not xgb_available:\n",
    "            raise RuntimeError(\"XGBoost not available\")\n",
    "            \n",
    "        try:\n",
    "            xgb_paths = [\n",
    "                self.artifacts_dir / 'xgboost_model.pkl',\n",
    "                self.artifacts_dir / 'xgb_model.pkl',\n",
    "                self.artifacts_dir / 'model_xgb.pkl'\n",
    "            ]\n",
    "            \n",
    "            for xgb_path in xgb_paths:\n",
    "                if xgb_path.exists():\n",
    "                    model = joblib.load(xgb_path)\n",
    "                    \n",
    "                    # Check for separate calibrator\n",
    "                    cal_path = self.artifacts_dir / 'xgb_calibrator.joblib'\n",
    "                    calibrator = None\n",
    "                    if cal_path.exists():\n",
    "                        calibrator = joblib.load(cal_path)\n",
    "                    \n",
    "                    self.models['XGBoost'] = {\n",
    "                        'model': model,\n",
    "                        'calibrator': calibrator,\n",
    "                        'type': 'XGBoost'\n",
    "                    }\n",
    "                    \n",
    "                    print(\"✓ XGBoost model loaded successfully\")\n",
    "                    return True\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load XGBoost model: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def load_best_model(self, preference=['CNN', 'XGBoost', 'sklearn']):\n",
    "        \"\"\"Load the best available model based on preference.\"\"\"\n",
    "        available = self.detect_available_models()\n",
    "        \n",
    "        if not available:\n",
    "            print(\"\\n✗ No models found in artifacts directory\")\n",
    "            return None\n",
    "            \n",
    "        # Load based on preference\n",
    "        for model_type in preference:\n",
    "            if model_type in available:\n",
    "                if model_type == 'CNN':\n",
    "                    if self.load_cnn_model():\n",
    "                        self.active_model = 'CNN'\n",
    "                        break\n",
    "                elif model_type == 'XGBoost':\n",
    "                    if self.load_xgboost_model():\n",
    "                        self.active_model = 'XGBoost'\n",
    "                        break\n",
    "                        \n",
    "        return self.active_model\n",
    "\n",
    "# Initialize model loader\n",
    "loader = ModelLoader()\n",
    "active_model = loader.load_best_model()\n",
    "\n",
    "if active_model:\n",
    "    print(f\"\\n✓ Active model: {active_model}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No models available, generating mock predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"Load test data from various sources.\"\"\"\n",
    "    \n",
    "    # Option 1: Load from processed test set\n",
    "    test_path = Path('../artifacts/test_set.npz')\n",
    "    if test_path.exists():\n",
    "        print(\"Loading test data from artifacts...\")\n",
    "        data = np.load(test_path)\n",
    "        return data['time'], data['flux'], data.get('labels')\n",
    "    \n",
    "    # Option 2: Load from TLS search results\n",
    "    tls_path = Path('../artifacts/denoised_lc.npz')\n",
    "    if tls_path.exists():\n",
    "        print(\"Loading data from TLS search results...\")\n",
    "        data = np.load(tls_path)\n",
    "        return data['time'], data['flux_denoised'], None\n",
    "    \n",
    "    # Option 3: Generate synthetic data\n",
    "    print(\"Generating synthetic test data...\")\n",
    "    return generate_synthetic_test_data()\n",
    "\n",
    "def generate_synthetic_test_data(n_samples=20):\n",
    "    \"\"\"Generate synthetic light curves for testing.\"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        has_transit = i % 2 == 0\n",
    "        \n",
    "        # Time array\n",
    "        n_points = 2048\n",
    "        t = np.linspace(0, 27.4, n_points)\n",
    "        \n",
    "        # Generate flux\n",
    "        flux = np.ones_like(t)\n",
    "        \n",
    "        # Transit parameters\n",
    "        period = np.random.uniform(2, 10)\n",
    "        t0 = np.random.uniform(0, 2)\n",
    "        duration = np.random.uniform(0.05, 0.15)\n",
    "        \n",
    "        if has_transit:\n",
    "            depth = np.random.uniform(0.0005, 0.002)\n",
    "            phase = ((t - t0) / period) % 1.0\n",
    "            in_transit = phase < (duration / period)\n",
    "            flux[in_transit] -= depth\n",
    "        \n",
    "        # Add noise\n",
    "        flux += np.random.normal(0, 5e-4, size=flux.shape)\n",
    "        \n",
    "        samples.append({\n",
    "            'time': t,\n",
    "            'flux': flux,\n",
    "            'period': period,\n",
    "            't0': t0,\n",
    "            'duration': duration,\n",
    "            'label': 1 if has_transit else 0,\n",
    "            'id': f'TIC_{100000 + i}'\n",
    "        })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load or generate test data\n",
    "test_samples = generate_synthetic_test_data()\n",
    "print(f\"\\nLoaded {len(test_samples)} test samples\")\n",
    "print(f\"Transit samples: {sum(s['label'] for s in test_samples)}\")\n",
    "print(f\"Non-transit samples: {sum(1-s['label'] for s in test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn_inference(model_dict, samples):\n",
    "    \"\"\"Run inference using CNN model.\"\"\"\n",
    "    from app.data.fold import make_views\n",
    "    \n",
    "    model = model_dict['model']\n",
    "    calibrator = model_dict['calibrator']\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in samples:\n",
    "            # Create global and local views\n",
    "            g_view, l_view = make_views(\n",
    "                sample['time'],\n",
    "                sample['flux'],\n",
    "                sample['period'],\n",
    "                sample['t0'],\n",
    "                sample['duration']\n",
    "            )\n",
    "            \n",
    "            # Convert to tensors\n",
    "            G = torch.tensor(g_view, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            L = torch.tensor(l_view, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get prediction\n",
    "            logits = model(G, L).squeeze()\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            \n",
    "            # Apply calibration\n",
    "            if calibrator is not None:\n",
    "                prob = calibrator.transform([[prob]])[0][0]\n",
    "            \n",
    "            predictions.append(float(prob))\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def run_xgboost_inference(model_dict, samples):\n",
    "    \"\"\"Run inference using XGBoost model.\"\"\"\n",
    "    model = model_dict['model']\n",
    "    calibrator = model_dict.get('calibrator')\n",
    "    predictions = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        # Extract features for XGBoost\n",
    "        features = extract_features_for_xgb(sample)\n",
    "        \n",
    "        # Get prediction\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            prob = model.predict_proba([features])[0][1]\n",
    "        else:\n",
    "            # For XGBoost native API\n",
    "            import xgboost as xgb\n",
    "            dtest = xgb.DMatrix([features])\n",
    "            prob = model.predict(dtest)[0]\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if calibrator is not None:\n",
    "            prob = calibrator.transform([[prob]])[0][0]\n",
    "            \n",
    "        predictions.append(float(prob))\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def extract_features_for_xgb(sample):\n",
    "    \"\"\"Extract features for XGBoost model.\"\"\"\n",
    "    flux = sample['flux']\n",
    "    \n",
    "    # Basic statistics\n",
    "    features = [\n",
    "        np.mean(flux),\n",
    "        np.std(flux),\n",
    "        np.min(flux),\n",
    "        np.max(flux),\n",
    "        np.median(flux),\n",
    "        np.percentile(flux, 25),\n",
    "        np.percentile(flux, 75),\n",
    "        sample['period'],\n",
    "        sample['duration'],\n",
    "    ]\n",
    "    \n",
    "    # Add more sophisticated features if needed\n",
    "    # - Fourier components\n",
    "    # - Autocorrelation\n",
    "    # - etc.\n",
    "    \n",
    "    return features\n",
    "\n",
    "def run_inference(loader, samples):\n",
    "    \"\"\"Run inference with the active model.\"\"\"\n",
    "    if loader.active_model == 'CNN':\n",
    "        print(\"Running CNN inference...\")\n",
    "        return run_cnn_inference(loader.models['CNN'], samples)\n",
    "    elif loader.active_model == 'XGBoost':\n",
    "        print(\"Running XGBoost inference...\")\n",
    "        return run_xgboost_inference(loader.models['XGBoost'], samples)\n",
    "    else:\n",
    "        print(\"Running mock inference...\")\n",
    "        # Mock predictions\n",
    "        return np.random.random(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "if loader.active_model:\n",
    "    predictions = run_inference(loader, test_samples)\n",
    "else:\n",
    "    print(\"⚠ Using random predictions (no model available)\")\n",
    "    predictions = np.random.random(len(test_samples))\n",
    "\n",
    "# Get true labels\n",
    "true_labels = np.array([s['label'] for s in test_samples])\n",
    "sample_ids = [s['id'] for s in test_samples]\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'id': sample_ids,\n",
    "    'probability': predictions,\n",
    "    'prediction': (predictions > 0.5).astype(int),\n",
    "    'true_label': true_labels\n",
    "})\n",
    "\n",
    "# Calculate correctness\n",
    "results_df['correct'] = results_df['prediction'] == results_df['true_label']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Inference Results:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "accuracy = results_df['correct'].mean()\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Predictions: {results_df['prediction'].sum()} planets detected\")\n",
    "print(f\"Ground truth: {results_df['true_label'].sum()} actual planets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "metrics = {\n",
    "    'model_type': loader.active_model or 'Mock',\n",
    "    'n_samples': len(test_samples),\n",
    "    'accuracy': accuracy_score(true_labels, results_df['prediction']),\n",
    "    'precision': precision_score(true_labels, results_df['prediction'], zero_division=0),\n",
    "    'recall': recall_score(true_labels, results_df['prediction'], zero_division=0),\n",
    "    'f1_score': f1_score(true_labels, results_df['prediction'], zero_division=0),\n",
    "}\n",
    "\n",
    "# Add ROC-AUC and PR-AUC if we have both classes\n",
    "if len(np.unique(true_labels)) > 1:\n",
    "    metrics['roc_auc'] = roc_auc_score(true_labels, predictions)\n",
    "    metrics['pr_auc'] = average_precision_score(true_labels, predictions)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, results_df['prediction'])\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{metric:15s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric:15s}: {value}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 No    Yes\")\n",
    "print(f\"Actual No    {cm[0,0]:5d} {cm[0,1]:5d}\")\n",
    "print(f\"       Yes   {cm[1,0]:5d} {cm[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Probability distribution by class\n",
    "transit_probs = predictions[true_labels == 1]\n",
    "no_transit_probs = predictions[true_labels == 0]\n",
    "\n",
    "axes[0,0].hist(no_transit_probs, bins=20, alpha=0.5, label='No Transit', color='blue', edgecolor='black')\n",
    "axes[0,0].hist(transit_probs, bins=20, alpha=0.5, label='Transit', color='red', edgecolor='black')\n",
    "axes[0,0].axvline(x=0.5, color='black', linestyle='--', label='Threshold')\n",
    "axes[0,0].set_xlabel('Predicted Probability')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].set_title('Distribution of Predicted Probabilities')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ROC Curve\n",
    "if len(np.unique(true_labels)) > 1:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[0,1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    axes[0,1].set_xlabel('False Positive Rate')\n",
    "    axes[0,1].set_ylabel('True Positive Rate')\n",
    "    axes[0,1].set_title('ROC Curve')\n",
    "    axes[0,1].legend(loc='lower right')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "if len(np.unique(true_labels)) > 1:\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    precision, recall, _ = precision_recall_curve(true_labels, predictions)\n",
    "    pr_auc = average_precision_score(true_labels, predictions)\n",
    "    \n",
    "    axes[1,0].plot(recall, precision, 'r-', linewidth=2, label=f'PR (AP = {pr_auc:.3f})')\n",
    "    axes[1,0].set_xlabel('Recall')\n",
    "    axes[1,0].set_ylabel('Precision')\n",
    "    axes[1,0].set_title('Precision-Recall Curve')\n",
    "    axes[1,0].legend(loc='lower left')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter plot of predictions\n",
    "colors = ['blue' if t == 0 else 'red' for t in true_labels]\n",
    "axes[1,1].scatter(range(len(predictions)), predictions, c=colors, alpha=0.6, s=50)\n",
    "axes[1,1].axhline(y=0.5, color='black', linestyle='--', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Sample Index')\n",
    "axes[1,1].set_ylabel('Predicted Probability')\n",
    "axes[1,1].set_title('Predictions by Sample')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', label='True Transit'),\n",
    "    Patch(facecolor='blue', label='True Non-Transit')\n",
    "]\n",
    "axes[1,1].legend(handles=legend_elements)\n",
    "\n",
    "plt.suptitle(f'Model Performance: {loader.active_model or \"Mock\"}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "reports_dir = Path('../reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "plt.savefig(reports_dir / 'inference_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Visualization saved to {reports_dir / 'inference_visualization.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output directory\n",
    "outputs_dir = Path('../outputs')\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save candidates CSV\n",
    "candidates_df = results_df[results_df['prediction'] == 1][['id', 'probability']].copy()\n",
    "candidates_df = candidates_df.rename(columns={'probability': 'confidence'})\n",
    "candidates_df['model'] = loader.active_model or 'Mock'\n",
    "\n",
    "candidates_file = outputs_dir / f'candidates_{timestamp}.csv'\n",
    "candidates_df.to_csv(candidates_file, index=False)\n",
    "print(f\"\\n✓ Saved {len(candidates_df)} candidates to {candidates_file}\")\n",
    "\n",
    "# Save all predictions\n",
    "all_predictions_file = outputs_dir / f'all_predictions_{timestamp}.csv'\n",
    "results_df.to_csv(all_predictions_file, index=False)\n",
    "print(f\"✓ Saved all predictions to {all_predictions_file}\")\n",
    "\n",
    "# Save provenance YAML\n",
    "provenance = {\n",
    "    'timestamp': timestamp,\n",
    "    'model': {\n",
    "        'type': loader.active_model or 'Mock',\n",
    "        'device': device if loader.active_model == 'CNN' else 'cpu',\n",
    "        'artifacts_dir': str(loader.artifacts_dir)\n",
    "    },\n",
    "    'data': {\n",
    "        'n_samples': len(test_samples),\n",
    "        'n_transits': int(true_labels.sum()),\n",
    "        'n_non_transits': int((1-true_labels).sum())\n",
    "    },\n",
    "    'results': {\n",
    "        'n_candidates': len(candidates_df),\n",
    "        'metrics': {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "                   for k, v in metrics.items()}\n",
    "    }\n",
    "}\n",
    "\n",
    "provenance_file = outputs_dir / f'provenance_{timestamp}.yaml'\n",
    "with open(provenance_file, 'w') as f:\n",
    "    yaml.dump(provenance, f, default_flow_style=False)\n",
    "print(f\"✓ Saved provenance to {provenance_file}\")\n",
    "\n",
    "# Save metrics JSON for dashboard\n",
    "metrics_file = reports_dir / f'inference_metrics_{loader.active_model or \"mock\"}.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"✓ Saved metrics to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"  Type: {loader.active_model or 'Mock'}\")\n",
    "print(f\"  Device: {device if loader.active_model == 'CNN' else 'CPU'}\")\n",
    "print(f\"  Artifacts: {loader.artifacts_dir}\")\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(test_samples)}\")\n",
    "print(f\"  Transit samples: {true_labels.sum()}\")\n",
    "print(f\"  Non-transit samples: {(1-true_labels).sum()}\")\n",
    "\n",
    "print(f\"\\nPrediction Results:\")\n",
    "print(f\"  Candidates detected: {len(candidates_df)}\")\n",
    "print(f\"  True positives: {((results_df['prediction'] == 1) & (results_df['true_label'] == 1)).sum()}\")\n",
    "print(f\"  False positives: {((results_df['prediction'] == 1) & (results_df['true_label'] == 0)).sum()}\")\n",
    "print(f\"  True negatives: {((results_df['prediction'] == 0) & (results_df['true_label'] == 0)).sum()}\")\n",
    "print(f\"  False negatives: {((results_df['prediction'] == 0) & (results_df['true_label'] == 1)).sum()}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.2%}\")\n",
    "if 'roc_auc' in metrics:\n",
    "    print(f\"  ROC-AUC: {metrics['roc_auc']:.3f}\")\n",
    "if 'pr_auc' in metrics:\n",
    "    print(f\"  PR-AUC: {metrics['pr_auc']:.3f}\")\n",
    "print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics['f1_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Candidates: {candidates_file}\")\n",
    "print(f\"  All predictions: {all_predictions_file}\")\n",
    "print(f\"  Provenance: {provenance_file}\")\n",
    "print(f\"  Metrics: {metrics_file}\")\n",
    "print(f\"  Visualization: {reports_dir / 'inference_visualization.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Inference pipeline completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a robust multi-model inference pipeline that:\n",
    "\n",
    "1. **Auto-detects available models**: CNN, XGBoost, or other models\n",
    "2. **Loads the best available model**: Based on user preference\n",
    "3. **Runs appropriate inference**: Different pipelines for different model types\n",
    "4. **Provides consistent output**: Same format regardless of model used\n",
    "5. **Comprehensive evaluation**: Metrics, visualizations, and exports\n",
    "\n",
    "The system gracefully handles missing models and dependencies, ensuring that inference can always proceed even if the preferred model is not available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}